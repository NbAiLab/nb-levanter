tokenizer: "mistralai/Mistral-7B-v0.1"
model:
  type: mistral

  seq_len: 2048
  hidden_dim: 4096
  intermediate_dim: 14336
  num_layers: 32
  num_heads: 32
  num_kv_heads: 8
  sliding_window: 2048

  activation_function: silu
  initializer_range: 0.02
  layer_norm_epsilon: 1e-05
  
  # upcast_attn: false
  use_flash_attention: true
  attn_backend: JAX_FLASH
  flash_attention_block_size: 1024
  # gradient_checkpointing: true
  # gradient_checkpointing_block_size: 5
  scan_layers: true
  # use_bias: false
  # rope_scaling: null
