data:
  id: "mimir-project/mimir-base"
  cache_dir: "gs://mimir-train-us-2/levanter/tokenized/mimir-control"
  tokenizer: "mimir-project/mimir-tokenizer-base"
model:
  type: llama

  num_layers: 27
  num_heads: 32
  hidden_dim: 2048
  intermediate_dim: 5504
  seq_len: 2048
  num_kv_heads: 32

  activation_function: silu
  initializer_range: 0.02
  layer_norm_epsilon: 1e-06

  # upcast_attn: false
  use_flash_attention: true
  attn_backend: JAX_FLASH
  # flash_attention_block_size: null
  # gradient_checkpointing: true
  # gradient_checkpointing_block_size: 5
  scan_layers: true
  # use_bias: false
  # rope_scaling: null
use_hf_model_config: false
trainer:
  wandb:
    entity: "nbailab"
    project: "Mímir Levanter - mimir-control"
    name: mimir-control
    tags: ["llama", "control", "7B", "scratch"]

  mp: p=f32,c=bfloat16
  train_batch_size: 2048  # Set to 2048 and parallelism 16, grad acc adjusted so 2048*2048=4194304, maybe 2040 here and 30 per_device  # 256 set for v4-64 TPU
  per_device_parallelism: 32  # anything smaller than 128 will trigger grad acc, but 128 won't fit in memory, so we decrease it
  per_device_eval_parallelism: 32
  num_train_steps: 30000  # Training for (2 epochs * 65748775870 tokens / (2048 seq length * 2048 batch size)) = 31351 steps
  steps_per_eval: 10000
  tensor_parallel_axes: ["mlp", "heads"]
  fsdp_axis: "embed"
  batch_axis: "batch"

  checkpointer:
    base_path: "gs://mimir-train-us-2/levanter/mimir-control/checkpoints"
    save_interval: 1h
    keep:
      - every: 10000

optimizer:
  lr_schedule: cosine
  learning_rate: 3e-4
  beta1: 0.9
  beta2: 0.95
  epsilon: 1e-8
  weight_decay: 0.1
  warmup: 0.0033  # ~100 steps
  min_lr_ratio: 0.1 # end lr 3e-5

hf_save_steps: 10000
hf_save_path: "gs://mimir-train-us-2/levanter/mimir-control/hf"
hf_upload: NbAiLab/mimir-control
