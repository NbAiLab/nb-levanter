tokenizer: "mimir-project/mimir-mistral-7b-base-scratch"
model:
  type: mistral

  num_layers: 32
  num_heads: 32
  hidden_dim: 1024
  intermediate_dim: 4096
  seq_len: 2048
  num_kv_heads: 8
  sliding_window: 2048

  activation_function: silu
  initializer_range: 0.02
  layer_norm_epsilon: 1e-05
  
  # upcast_attn: false
  use_flash_attention: true
  attn_backend: JAX_FLASH
  flash_attention_block_size: 1024
  # gradient_checkpointing: true
  # gradient_checkpointing_block_size: 5
  scan_layers: true
  # use_bias: false
  # rope_scaling: null